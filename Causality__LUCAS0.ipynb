{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaNardi/Causality/blob/main/Causality__LUCAS0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ISPR Assignment N.5\n",
        "\n",
        "\n",
        "\n",
        "In this assignment, you will focus on recovering a causal graph from observational data and estimating the causal effect of a treatment variable on an outcome variable over a synthetic dataset.\n",
        "\n",
        "`LUCAS0` is a simulated dataset representing the interaction between smoking, lung cancer, and other related variables. The [website](https://www.causality.inf.ethz.ch/data/LUCAS.html) of the dataset also reports the ground-truth graph and the ground-truth values for the conditional probabilities, which you can use to verify your results.\n",
        "\n",
        "The assignment requires you to perform the following causal analysis:\n",
        "\n",
        "1. Perform **structure learning** by applying the `PC` algorithm to the provided dataset and recover the CPDAG. The suggestion is to use the implementation of the `PC` algorithm from the [`dowhy`](https://www.pywhy.org/dowhy/v0.12/) library.\n",
        "2. If there are unoriented edges, suppose to have expert knowledge and reconstruct the causal graphs by orienting them as in the **ground truth** graphs provided with the dataset.\n",
        "3. After fixing the structure, you need to **fit the parameters** of the network, for which you can use any of the existing libraries for probabilistic model such as `pgmpy`, `pomegranate`, `bnlearn`, `PyMC`, `Pyro`, `Stan`, or even implement yourself a representation of the Conditional Probability Tables.\n",
        "4. Let $X$ be the treatment variable and $Y$ the outcome. Compute the **Average Treatment Effect** (ATE) of $X$ on $Y$ $$\\mathbb{E}[Y\\mid \\text{do}(X=1)]- \\mathbb{E}[Y\\mid \\text{do}(X=0)],$$ by finding a valid adjustment set $Z$. Detail why the adjustment set is valid to estimate the causal effect of $X$ on $Y$. For the `LUCAS0` dataset take $X=\\text{Lung Cancer}$ and $Y=\\text{Car Accident}$.\n",
        "5. Finally, compute the difference between the conditional probabilities $$\\mathbb{E}[Y\\mid X=1]- \\mathbb{E}[Y\\mid X=0],$$ and discuss the results against those from the ATE."
      ],
      "metadata": {
        "id": "VtmRcQok7_-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "\n",
        "Install the libraries `dowhy` and `pgmpy` to respectively perform structure learning and parameter fitting of the Causal Bayesian Network of the `LUCAS0` dataset."
      ],
      "metadata": {
        "id": "EGt1c7f6-yzo"
      }
    },
   
    {
      "cell_type": "markdown",
      "source": [
        "Seed everything for reproducibility purposes and download the dataset."
      ],
      "metadata": {
        "id": "_177i2bZUyo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  print(f\"All random number generators seeded with {seed}\")\n",
        "\n",
        "# Seed\n",
        "seed_everything()\n",
        "\n",
        "# Load Data\n",
        "url = \"http://www.causality.inf.ethz.ch/data/lucas0_train.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# since casuallearn requires npy arrays\n",
        "data_np = data.to_numpy()\n",
        "attributes = list(data.columns)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "za6NVIGFybXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structure Learning\n",
        "\n",
        "Given the provided `LUCAS0` dataset retrieve the causal structure using the `PC` algorithm from the `dowhy` library. As the results of the algorithm strongly suggests on the ordering in which the adjustment sets are chosen, it is strongly recommended to fix the seed before the execution for reproducibility.\n",
        "\n",
        "After the run of the algorithm, discuss the presence of unoriented edges and then orient them by using the provided ground-truth graph."
      ],
      "metadata": {
        "id": "kQiMq9cuVV-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from causallearn.graph.Graph import Graph\n",
        "from causallearn.graph.Edge import Edge\n",
        "from causallearn.graph.Endpoint import Endpoint\n",
        "from causallearn.search.ConstraintBased.PC import pc\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "import io\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CcubU7L3KYC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting with graphs\n",
        "We start by trying out various combination of hyperparameters to see what kind of graphs we get\n",
        "we convert to pydot because that seems the most efficient way to plot the graph in casuallearn\n",
        "it is not very interesting to change the name of the nodes in the causal graph itself as they are named by the order of the npy array (which is also the order we immediatly find on the LUCAS website)\n",
        "\n",
        "We plot the graphs to look at the depending on the hyperparameters setting we get graphs that are more or less correct the selection is based on my knowledge, which in this specific instance was very close to the expert knowledge (which i will anyway obtain in the next step)"
      ],
      "metadata": {
        "id": "o3v24h9AYE8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lists of hyperparameters to use in testing\n",
        "rule=0\n",
        "test=\"fisherz\"\n",
        "alphas=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
        "tests=[\"fisherz\",\"chisq\",\"gsq\"]\n",
        "\n",
        "# alphas are the more interesting, we interacting with the other hyperparameters\n",
        "# one at a time but always check multiple alphas for all of them\n",
        "for alph in alphas:\n",
        "\n",
        "# other hyperparameter values have been commented\n",
        "\n",
        "  #for rule in range(0,3):\n",
        "  #for test in tests:\n",
        "    # this function builds the causal graph based on the data and our hyperparameters\n",
        "    cg = pc(data_np, alpha=alph,indep_test=test, uc_rule=rule, verbose=False)\n",
        "\n",
        "    # we convert to pydot because that seems the most efficient way to plot the\n",
        "    # graph in casuallearn\n",
        "    pyd = GraphUtils.to_pydot(cg.G)\n",
        "\n",
        "    # we change the names of the nodes so that the graph is readable\n",
        "    for node in pyd.get_nodes():\n",
        "        index = int(node.get_name())\n",
        "        node.set_label(attributes[index])\n",
        "\n",
        "    # we plot the graphs to look at them\n",
        "    tmp_png = pyd.create_png(f=\"png\")\n",
        "    fp = io.BytesIO(tmp_png)\n",
        "    img = mpimg.imread(fp, format='png')\n",
        "    print(\"alpha =\", alph, \", inep_test =\", test, \", uc_rule= \", rule)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EPOhedWr1FhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best graph\n",
        "Given my preferred graph we pick its parameter settings.\n",
        "\n",
        "(an alpha of 0.1 actually generated the same graph)\n"
      ],
      "metadata": {
        "id": "_NN-PSjcYQ86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given my preferred graph we pick its parameter settings\n",
        "cg = pc(data_np, alpha=0.05, uc_rule=0, verbose=False)\n",
        "\n",
        "# this code is the same as the one in the previous cell\n",
        "pyd = GraphUtils.to_pydot(cg.G)\n",
        "for node in pyd.get_nodes():\n",
        "\n",
        "    index = int(node.get_name())\n",
        "    node.set_label(attributes[index])\n",
        "\n",
        "\n",
        "tmp_png = pyd.create_png(f=\"png\")\n",
        "fp = io.BytesIO(tmp_png)\n",
        "img = mpimg.imread(fp, format='png')\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ncIyowhkKkkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orient undirected edges\n",
        "\n",
        "We use the expert knowledge to orient the only unoriented edge\n",
        "we can see that the graph is now oriented\n"
      ],
      "metadata": {
        "id": "G5JCIa-3YcEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we add a directed edge between genetics and attention disorder\n",
        "cg.G.add_directed_edge(cg.G.nodes [attributes.index(\"Genetics\")],cg.G.nodes[attributes.index(\"Attention_Disorder\")])\n",
        "\n",
        "\n",
        "# we use the same code as before to add labels and plot\n",
        "pyd = GraphUtils.to_pydot(cg.G)\n",
        "\n",
        "for node in pyd.get_nodes():\n",
        "    index = int(node.get_name())\n",
        "    node.set_label(attributes[index])\n",
        "tmp_png = pyd.create_png(f=\"png\")\n",
        "fp = io.BytesIO(tmp_png)\n",
        "img = mpimg.imread(fp, format='png')\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# we can see that the graph is now oriented\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gX-gZuxsbk3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Fitting\n",
        "\n",
        "Given the structure of the Causal Bayesian Network, you need to fit the parameters from data. Recall that in a Bayesian Network the parameters are stored in the Conditional Probability Tables that determine the probability of each variable given its parents in the graph. The goal is then to fill this table. As before, you can validate your results with the ground truth reported in the `LUCAS0` website. However, your implementation should **clearly show** that you fitted the parameters from data."
      ],
      "metadata": {
        "id": "ukk6kTT-VaeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pylab as plt"
      ],
      "metadata": {
        "id": "1Hyp5OlpnG5i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pgmpy model\n",
        "I have not found a way to turn a causallearn graph (or a pydot) into an nx\n",
        "graph or a pgmpy network. I am confident there is a simple way to do that as this question seems to not been asked on the internet so far.\n",
        "\n",
        "A very large part of the effort put into this project was trying to find out\n",
        "what library is used as an intermediate language to move DAGs between\n",
        "the other ones and while nx seems the most likely candidate such unification has (probably) not happened yet.\n",
        "\n",
        "nx methods should be callable on pgmpy but that doesn't seem to be very\n",
        "consistent. We still managed to build a graph and plot it to check whether there has been some error in moving the graph by hand.\n",
        "\n",
        "born_an_even_day is not plotted because it has no edges but we did add it\n",
        "to the model explicitely so there is no need to double check using the plot"
      ],
      "metadata": {
        "id": "opKQYjrYbFH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We manually copy all the edges from the previously discovered graph\n",
        "edges = [\n",
        "    ('Smoking', 'Lung_cancer'),\n",
        "    ('Peer_Pressure', 'Smoking'),\n",
        "    ('Smoking', 'Yellow_Fingers'),\n",
        "    ('Genetics', 'Lung_cancer'),\n",
        "    ('Genetics', 'Attention_Disorder'),\n",
        "    ('Attention_Disorder', 'Car_Accident'),\n",
        "    ('Lung_cancer', 'Coughing'),\n",
        "    ('Lung_cancer', 'Fatigue'),\n",
        "    ('Fatigue', 'Car_Accident'),\n",
        "    ('Anxiety', 'Smoking'),\n",
        "    ('Coughing', 'Fatigue'),\n",
        "    ('Allergy', 'Coughing')\n",
        "]\n",
        "\n",
        "# We turn our edge list into a model\n",
        "model = DiscreteBayesianNetwork (edges)\n",
        "model.add_node('Born_an_Even_Day')\n",
        "\n",
        "# We inspect the graph\n",
        "print(list(model.nodes()))\n",
        "print(list(model.edges()))\n",
        "\n",
        "# We plot the graph\n",
        "nx_graph = nx.DiGraph(model.edges())\n",
        "nx.draw(nx_graph, with_labels=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R3fgAwpuSbJ3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute cdps"
      ],
      "metadata": {
        "id": "TU8e_fyebKRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#given the model pgmpy is pretty straightforward in its computation of the cpds\n",
        "model.fit(data)\n",
        "\n",
        "# we print the cdps to compare them with the ground truth.\n",
        "for cpd in model.get_cpds():\n",
        "    print(f\"CPD of {cpd.variable}:\")\n",
        "    print(cpd)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UgWHle5i6MjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####Table with the cdps taken from LUCAS against the ones we found out.\n",
        "The largest difference (0.07545) is in P(Fatigue=T|Lung cancer=F, Coughing=T)\n",
        "that is pretty big, but the other ones don't go above 0.03.\n",
        "The results overall seem satisfying\n",
        "\n",
        "P(Anxiety=T)=0.64277\t                              0.6305\n",
        "\n",
        "P(Peer Pressure=T)=0.32997\t                      \t0.3415\n",
        "\n",
        "P(Smoking=T|Peer Pressure=F, Anxiety=F)=0.43118\t\t  0.430327868852459\n",
        "\n",
        "P(Smoking=T|Peer Pressure=T, Anxiety=F)=0.74591\t\t  0.7131474103585658\n",
        "\n",
        "P(Smoking=T|Peer Pressure=F, Anxiety=T)=0.8686\t\t  0.8685162846803377\n",
        "\n",
        "P(Smoking=T|Peer Pressure=T, Anxiety=T)=0.91576\t\t  0.9166666666666666\n",
        "\n",
        "\n",
        "P(Yellow Fingers=T|Smoking=F)=0.23119               0.22424242424242424\n",
        "\n",
        "P(Yellow Fingers=T|Smoking=T)=0.95372               0.9654485049833887\n",
        "\n",
        "P(Genetics=T)=0.15953                               0.1395\n",
        "\n",
        "P(Lung cancer=T|Genetics=F, Smoking=F)=0.23146\t\t\t0.2517482517482518\n",
        "\n",
        "P(Lung cancer=T|Genetics=T, Smoking=F)=0.86996\t\t\t0.8939393939393939\n",
        "\n",
        "P(Lung cancer=T|Genetics=F, Smoking=T)=0.83934\t\t\t0.8227554179566563\n",
        "\n",
        "P(Lung cancer=T|Genetics=T, Smoking=T)=0.99351\t\t  1.0\n",
        "\n",
        "P(Attention Disorder=T|Genetics=F)=0.28956\t\t      0.27193492155723414\n",
        "\n",
        "P(Attention Disorder=T|Genetics=T)=0.68706\t\t      0.6344086021505376\n",
        "\n",
        "P(Born an Even Day=T)=0.5\t\t\t                      0.4895\n",
        "\n",
        "P(Allergy=T)=0.32841\t\t\t                          0.343\n",
        "\n",
        "P(Coughing=T|Allergy=F, Lung cancer=F)=0.1347\t\t    0.1352112676056338\n",
        "\n",
        "P(Coughing=T|Allergy=T, Lung cancer=F)=0.64592\t\t  0.6435643564356436\n",
        "\n",
        "P(Coughing=T|Allergy=F, Lung cancer=T)=0.7664\t\t    0.7705943691345151\n",
        "\n",
        "P(Coughing=T|Allergy=T, Lung cancer=T)=0.99947\t\t\t1.0\n",
        "\n",
        "P(Fatigue=T|Lung cancer=F, Coughing=F)=0.35212\t\t  0.35883905013192613\n",
        "\n",
        "P(Fatigue=T|Lung cancer=T, Coughing=F)=0.56514\t\t  0.5454545454545454\n",
        "\n",
        "P(Fatigue=T|Lung cancer=F, Coughing=T)=0.80016\t\t  0.7247191011235955\n",
        "\n",
        "P(Fatigue=T|Lung cancer=T, Coughing=T)=0.89589\t\t  0.8904333605887162\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=F, Fatigue=F)=0.2274\t\t  0.21351351351351353\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=T, Fatigue=F)=0.779\t\t    0.7628205128205128\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=F, Fatigue=T)=0.78861\t\t\t0.7857868020304568\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=T, Fatigue=T)=0.97169\t\t  0.9693251533742331\n"
      ],
      "metadata": {
        "id": "g7CTTDECNTRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional and Interventional Estimate\n",
        "\n",
        "What is a valid adjustment set to estimate the causal effect of `Lung_cancer` on `Car Accident`? Justify your choice and compute the Average Treatment Effect. Then, compute the difference between conditional expectations and discuss whether they agree or not."
      ],
      "metadata": {
        "id": "YrQiK-ThVfKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dowhy import CausalModel"
      ],
      "metadata": {
        "id": "zVU1BCFgIA_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The adjustment set is composed only by the node Genetics, as it is the only parent of X that allows for a path to Y which doesn't include X (that is, the one through attention_disorder)"
      ],
      "metadata": {
        "id": "1IXUQLbJFjFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Car Accident conditioned only on Lung cancer\n",
        "We compute the difference between the average value of car accident given lung cancer and given no lung cancer and check the difference between those values\n",
        "\n",
        "$E[\\text{CarAccident} \\mid \\text{LungCancer = 1}]-E[\\text{CarAccident} \\mid \\text{LungCancer = 0}]$"
      ],
      "metadata": {
        "id": "Pw6z6oHvbXVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the conditional means\n",
        "mean_Y_given_X1 = data.loc[data['Lung_cancer'] == 1, 'Car_Accident'].mean()\n",
        "mean_Y_given_X0 = data.loc[data['Lung_cancer'] == 0, 'Car_Accident'].mean()\n",
        "\n",
        "# Compute the difference between the two\n",
        "conditional_diff = mean_Y_given_X1 - mean_Y_given_X0\n",
        "print(mean_Y_given_X1)\n",
        "print(mean_Y_given_X0)\n",
        "print(conditional_diff)"
      ],
      "metadata": {
        "id": "jT4BvJHLwfMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATE computation using Genetics\n",
        "We compute\n",
        "\n",
        "$\\sum_{w \\in \\{0,1\\}} E[\\text{CarAccident} \\mid \\text{LungCancer = 1}, \\text{Genetics}=w] \\cdot P(\\text{Genetics}=w)$\n",
        "\n",
        "and\n",
        "\n",
        "$\\sum_{w \\in \\{0,1\\}} E[\\text{CarAccident} \\mid \\text{LungCancer = 0}, \\text{Genetics}=w] \\cdot P(\\text{Genetics}=w)$\n",
        "\n",
        "Using the values from the dataset: the difference between them is the average treatment effect, adjusted for genetics"
      ],
      "metadata": {
        "id": "SF8P58yfbfmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We compute a matrix of lung cancer and genetics 0s and 1s and count how many points\n",
        "# have car accident = 1 in each square of the matrix\n",
        "X1_G1 = data.loc[(data['Lung_cancer'] == 1) & (data['Genetics'] == 1), 'Car_Accident'].mean()\n",
        "X1_G0 = data.loc[(data['Lung_cancer'] == 1) & (data['Genetics'] == 0), 'Car_Accident'].mean()\n",
        "X0_G1 = data.loc[(data['Lung_cancer'] == 0) & (data['Genetics'] == 1), 'Car_Accident'].mean()\n",
        "X0_G0 = data.loc[(data['Lung_cancer'] == 0) & (data['Genetics'] == 0), 'Car_Accident'].mean()\n",
        "\n",
        "# We compute the marginal distribution of Genetics\n",
        "P_G1 = (data['Genetics'] == 1).mean()\n",
        "P_G0 = 1 - P_G1\n",
        "\n",
        "# We adjust expectations according to the distribution of Genetics\n",
        "Y_do_X1 = X1_G1 * P_G1 + X1_G0 * P_G0\n",
        "Y_do_X0 = X0_G1 * P_G1 + X0_G0 * P_G0\n",
        "\n",
        "# We compute the difference between the points where\n",
        "adjusted_ate = Y_do_X1 - Y_do_X0\n",
        "print(Y_do_X1)\n",
        "print(Y_do_X0)\n",
        "print(adjusted_ate)"
      ],
      "metadata": {
        "id": "v5exmb_Ehryz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The result we just got from our ATE computation is extremely close to the one we had gotten without conditioning on genetics\n",
        "\n",
        "This implies that Genetics is not actually a good adjustment set (or that there is very little effect to adjust for), as we had hypothized"
      ],
      "metadata": {
        "id": "PMIELmzeMAL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we count in absolute terms the relationships between genetics and lung cancer\n",
        "print(len(data.loc[(data['Genetics'] == 1) & (data['Lung_cancer'] == 0)]))\n",
        "print(len(data.loc[(data['Genetics'] == 1) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Genetics'] == 0) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Genetics'] == 0) & (data['Lung_cancer'] == 0)]))\n"
      ],
      "metadata": {
        "id": "Y38Q0_yskbaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the vast majority of values in which Genetics is 1 also have lung cancer equal to 1. This might not be the best sign of a good adjustment set.\n",
        "Since attention disorder also blocks the backdoor path (as it is the only other child of genetics), let's try exploring it too"
      ],
      "metadata": {
        "id": "z15W55GpQtwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we count in absolute terms the relationships between lung cancer and attention disorder\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 1) & (data['Lung_cancer'] == 0)]))\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 1) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 0) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 0) & (data['Lung_cancer'] == 0)]))"
      ],
      "metadata": {
        "id": "zuovi5_aNEOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATE using attention disorder\n",
        "Attention disorder seems to not share the same problem as genetics. Let's try running the same code as before using it as our adjustment set"
      ],
      "metadata": {
        "id": "ZfSwlwWLVphu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We compute a matrix of lung cancer and attention disorder 0s and 1s and count how many points\n",
        "# have car accident = 1 in each square of the matrix\n",
        "X1_A1 = data.loc[(data['Lung_cancer'] == 1) & (data['Attention_Disorder'] == 1), 'Car_Accident'].mean()\n",
        "X1_A0 = data.loc[(data['Lung_cancer'] == 1) & (data['Attention_Disorder'] == 0), 'Car_Accident'].mean()\n",
        "X0_A1 = data.loc[(data['Lung_cancer'] == 0) & (data['Attention_Disorder'] == 1), 'Car_Accident'].mean()\n",
        "X0_A0 = data.loc[(data['Lung_cancer'] == 0) & (data['Attention_Disorder'] == 0), 'Car_Accident'].mean()\n",
        "\n",
        "# We compute the marginal distribution of attention disorder\n",
        "P_A1 = (data['Attention_Disorder'] == 1).mean()\n",
        "P_A0 = 1 - P_G1\n",
        "\n",
        "# We adjust expectations according to the distribution of attention disorder\n",
        "Y_do_X1 = X1_A1 * P_A1 + X1_A0 * P_A0\n",
        "Y_do_X0 = X0_A1 * P_A1 + X0_A0 * P_A0\n",
        "\n",
        "# We compute the difference between the points where\n",
        "adjusted_ate = Y_do_X1 - Y_do_X0\n",
        "print(Y_do_X1)\n",
        "print(Y_do_X0)\n",
        "print(adjusted_ate)"
      ],
      "metadata": {
        "id": "heGVbiBRLN7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effect now is slightly lower than the one we had previously estimated using genetics. It is still very similar to the naive one but not uncanningly so"
      ],
      "metadata": {
        "id": "6ts2cPf4V-6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATE estimation using dowhy\n",
        "\n",
        " dowhy has its own ate function, so we try it to compare its results with ours\n",
        " we first need to build the model. For some reason i'm unable to utilize the\n",
        " one i had originally built during the structure learning (from my understanding casuallearn is part of pywhy) but luckily CausalModel accepts nx graphs as input\n",
        "\n",
        " Printing the result confirms us that genetics was indeed the adjustment_set\n",
        " frontdoor is another method that is explained in the book that requires\n",
        " a different adjustment set from the one we use in backdoor\n",
        "\n"
      ],
      "metadata": {
        "id": "4swTifrTbUkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we build the dowhy causalmodel using our graph and data\n",
        "modeldw = CausalModel(\n",
        "    data=data,\n",
        "    treatment='Lung_cancer',\n",
        "    outcome='Car_Accident',\n",
        "    graph=nx_graph\n",
        ")\n",
        "\n",
        "# i don't really understand why but the estimate is a required input in\n",
        "# the estimate_effect method, which we will use later.\n",
        "# running this function also helps us confirm our adjustment sets\n",
        "estimand = modeldw.identify_effect()\n",
        "\n",
        "# we print this\n",
        "print(estimand)\n"
      ],
      "metadata": {
        "id": "cfK1vieqwYzG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " We try both the backdoor and frontdoor methods as they are already implemented in dowhy\n"
      ],
      "metadata": {
        "id": "hS8X-v1vaUGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we run the estimation function using genetics, as suggested by the estimand\n",
        "estimatebackgen = modeldw.estimate_effect(\n",
        "    estimand,\n",
        "    method_name=\"backdoor.linear_regression\",\n",
        "    method_params={\"backdoor_adjustment_set\": [\"Genetics\"]}\n",
        ")\n",
        "\n",
        "# we also run using attention disorder, as it had given us more promising results\n",
        "estimatebackad = modeldw.estimate_effect(\n",
        "    estimand,\n",
        "    method_name=\"backdoor.linear_regression\",\n",
        "    method_params={\"backdoor_adjustment_set\": [\"Attention_Disorder\"]}\n",
        ")\n",
        "\n",
        "# we run the frontdoor effect estimation function using the values we obtained\n",
        "# from identify_effect\n",
        "estimatefront = modeldw.estimate_effect(\n",
        "    estimand,\n",
        "    method_name=\"frontdoor.linear_regression\",\n",
        "    method_params={\"frontdoor_adjustment_set\": [\"Coughing\",  \"Fatigue\"]}\n",
        ")\n",
        "\n",
        "# we print the results\n",
        "print(estimatebackgen.value, '\\n', estimatebackad.value, '\\n', estimatefront.value)"
      ],
      "metadata": {
        "id": "QecAg1BMaS-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All functions give us the same result.\n",
        "In the backdoor method the algorithm is probably able to use whatever set completely blocks the path, which makes this result easily predictable, while in the frontdoor method it is more interesting to see it get the same results.\n",
        "\n",
        "All results are also generally much closer to the one we had found using attention disorder as our adjustment set.\n"
      ],
      "metadata": {
        "id": "kDumGkDqWXVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions\n",
        "Despite my initial enthusiasm genetics proved itself not to be the correct adjustment set. The fact that the edge between it and attention disorder had to be manually directed only prompted me to think that there is a weak causal relationship between genetics and attention disorder, which in my eyes explained the weak effect genetics had as an adjustment set. Had i been more open minded i would have lost much less time exploring libraries.\n",
        "\n",
        " Regarding the slight difference between dowhy's and my estimations, given that we use the same data and refer to the same function in our computation, there must be something something i don't expect to be happening in the linear regression method of dowhy (looking at the linear_regression_estimator function, which is called in dowhy/causal_model it is unclear what the library does with this linear estimator but given the nature of the problem there are not many guesses). This is a problem i explored too deeply before trying out attention disorder as an adjustment set, but it might still be somewhat interesting to note.\n"
      ],
      "metadata": {
        "id": "YbPqaCsuJPon"
      }
    }
  ]
}
