{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaNardi/Causality/blob/main/Causality__LUCAS0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ISPR Assignment N.5\n",
        "\n",
        "\n",
        "\n",
        "In this assignment, you will focus on recovering a causal graph from observational data and estimating the causal effect of a treatment variable on an outcome variable over a synthetic dataset.\n",
        "\n",
        "`LUCAS0` is a simulated dataset representing the interaction between smoking, lung cancer, and other related variables. The [website](https://www.causality.inf.ethz.ch/data/LUCAS.html) of the dataset also reports the ground-truth graph and the ground-truth values for the conditional probabilities, which you can use to verify your results.\n",
        "\n",
        "The assignment requires you to perform the following causal analysis:\n",
        "\n",
        "1. Perform **structure learning** by applying the `PC` algorithm to the provided dataset and recover the CPDAG. The suggestion is to use the implementation of the `PC` algorithm from the [`dowhy`](https://www.pywhy.org/dowhy/v0.12/) library.\n",
        "2. If there are unoriented edges, suppose to have expert knowledge and reconstruct the causal graphs by orienting them as in the **ground truth** graphs provided with the dataset.\n",
        "3. After fixing the structure, you need to **fit the parameters** of the network, for which you can use any of the existing libraries for probabilistic model such as `pgmpy`, `pomegranate`, `bnlearn`, `PyMC`, `Pyro`, `Stan`, or even implement yourself a representation of the Conditional Probability Tables.\n",
        "4. Let $X$ be the treatment variable and $Y$ the outcome. Compute the **Average Treatment Effect** (ATE) of $X$ on $Y$ $$\\mathbb{E}[Y\\mid \\text{do}(X=1)]- \\mathbb{E}[Y\\mid \\text{do}(X=0)],$$ by finding a valid adjustment set $Z$. Detail why the adjustment set is valid to estimate the causal effect of $X$ on $Y$. For the `LUCAS0` dataset take $X=\\text{Lung Cancer}$ and $Y=\\text{Car Accident}$.\n",
        "5. Finally, compute the difference between the conditional probabilities $$\\mathbb{E}[Y\\mid X=1]- \\mathbb{E}[Y\\mid X=0],$$ and discuss the results against those from the ATE."
      ],
      "metadata": {
        "id": "VtmRcQok7_-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "\n",
        "Install the libraries `dowhy` and `pgmpy` to respectively perform structure learning and parameter fitting of the Causal Bayesian Network of the `LUCAS0` dataset."
      ],
      "metadata": {
        "id": "EGt1c7f6-yzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --clear-output --inplace your_notebook.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qexohSwyBQuD",
        "outputId": "e0c93cd7-4830-4d0e-e20b-c479c0d380d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern 'your_notebook.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--coalesce-streams\n",
            "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    Overwrite base name use for output files.\n",
            "                Supports pattern replacements '{notebook_name}'.\n",
            "    Default: '{notebook_name}'\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed everything for reproducibility purposes and download the dataset."
      ],
      "metadata": {
        "id": "_177i2bZUyo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  print(f\"All random number generators seeded with {seed}\")\n",
        "\n",
        "# Seed\n",
        "seed_everything()\n",
        "\n",
        "# Load Data\n",
        "url = \"http://www.causality.inf.ethz.ch/data/lucas0_train.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# since casuallearn requires npy arrays\n",
        "data_np = data.to_numpy()\n",
        "attributes = list(data.columns)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "za6NVIGFybXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structure Learning\n",
        "\n",
        "Given the provided `LUCAS0` dataset retrieve the causal structure using the `PC` algorithm from the `dowhy` library. As the results of the algorithm strongly suggests on the ordering in which the adjustment sets are chosen, it is strongly recommended to fix the seed before the execution for reproducibility.\n",
        "\n",
        "After the run of the algorithm, discuss the presence of unoriented edges and then orient them by using the provided ground-truth graph."
      ],
      "metadata": {
        "id": "kQiMq9cuVV-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from causallearn.graph.Graph import Graph\n",
        "from causallearn.graph.Edge import Edge\n",
        "from causallearn.graph.Endpoint import Endpoint\n",
        "from causallearn.search.ConstraintBased.PC import pc\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "import io\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CcubU7L3KYC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting with graphs\n",
        "We start by trying out various combination of hyperparameters to see what kind of graphs we get\n",
        "we convert to pydot because that seems the most efficient way to plot the graph in casuallearn\n",
        "it is not very interesting to change the name of the nodes in the causal graph itself as they are named by the order of the npy array (which is also the order we immediatly find on the LUCAS website)\n",
        "\n",
        "We plot the graphs to look at the depending on the hyperparameters setting we get graphs that are more or less correct the selection is based on my knowledge, which in this specific instance was very close to the expert knowledge (which i will anyway obtain in the next step)"
      ],
      "metadata": {
        "id": "o3v24h9AYE8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lists of hyperparameters to use in testing\n",
        "rule=0\n",
        "test=\"fisherz\"\n",
        "alphas=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
        "tests=[\"fisherz\",\"chisq\",\"gsq\"]\n",
        "\n",
        "# alphas are the more interesting, we interacting with the other hyperparameters\n",
        "# one at a time but always check multiple alphas for all of them\n",
        "for alph in alphas:\n",
        "\n",
        "# other hyperparameter values have been commented\n",
        "\n",
        "  #for rule in range(0,3):\n",
        "  #for test in tests:\n",
        "    # this function builds the causal graph based on the data and our hyperparameters\n",
        "    cg = pc(data_np, alpha=alph,indep_test=test, uc_rule=rule, verbose=False)\n",
        "\n",
        "    # we convert to pydot because that seems the most efficient way to plot the\n",
        "    # graph in casuallearn\n",
        "    pyd = GraphUtils.to_pydot(cg.G)\n",
        "\n",
        "    # we change the names of the nodes so that the graph is readable\n",
        "    for node in pyd.get_nodes():\n",
        "        index = int(node.get_name())\n",
        "        node.set_label(attributes[index])\n",
        "\n",
        "    # we plot the graphs to look at them\n",
        "    tmp_png = pyd.create_png(f=\"png\")\n",
        "    fp = io.BytesIO(tmp_png)\n",
        "    img = mpimg.imread(fp, format='png')\n",
        "    print(\"alpha =\", alph, \", inep_test =\", test, \", uc_rule= \", rule)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EPOhedWr1FhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best graph\n",
        "Given my preferred graph we pick its parameter settings.\n",
        "\n",
        "(an alpha of 0.1 actually generated the same graph)\n"
      ],
      "metadata": {
        "id": "_NN-PSjcYQ86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given my preferred graph we pick its parameter settings\n",
        "cg = pc(data_np, alpha=0.05, uc_rule=0, verbose=False)\n",
        "\n",
        "# this code is the same as the one in the previous cell\n",
        "pyd = GraphUtils.to_pydot(cg.G)\n",
        "for node in pyd.get_nodes():\n",
        "\n",
        "    index = int(node.get_name())\n",
        "    node.set_label(attributes[index])\n",
        "\n",
        "\n",
        "tmp_png = pyd.create_png(f=\"png\")\n",
        "fp = io.BytesIO(tmp_png)\n",
        "img = mpimg.imread(fp, format='png')\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ncIyowhkKkkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orient undirected edges\n",
        "\n",
        "We use the expert knowledge to orient the only unoriented edge\n",
        "we can see that the graph is now oriented\n"
      ],
      "metadata": {
        "id": "G5JCIa-3YcEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we add a directed edge between genetics and attention disorder\n",
        "cg.G.add_directed_edge(cg.G.nodes [attributes.index(\"Genetics\")],cg.G.nodes[attributes.index(\"Attention_Disorder\")])\n",
        "\n",
        "\n",
        "# we use the same code as before to add labels and plot\n",
        "pyd = GraphUtils.to_pydot(cg.G)\n",
        "\n",
        "for node in pyd.get_nodes():\n",
        "    index = int(node.get_name())\n",
        "    node.set_label(attributes[index])\n",
        "tmp_png = pyd.create_png(f=\"png\")\n",
        "fp = io.BytesIO(tmp_png)\n",
        "img = mpimg.imread(fp, format='png')\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# we can see that the graph is now oriented\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gX-gZuxsbk3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Fitting\n",
        "\n",
        "Given the structure of the Causal Bayesian Network, you need to fit the parameters from data. Recall that in a Bayesian Network the parameters are stored in the Conditional Probability Tables that determine the probability of each variable given its parents in the graph. The goal is then to fill this table. As before, you can validate your results with the ground truth reported in the `LUCAS0` website. However, your implementation should **clearly show** that you fitted the parameters from data."
      ],
      "metadata": {
        "id": "ukk6kTT-VaeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pylab as plt"
      ],
      "metadata": {
        "id": "1Hyp5OlpnG5i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pgmpy model\n",
        "I have not found a way to turn a causallearn graph (or a pydot) into an nx\n",
        "graph or a pgmpy network. I am confident there is a simple way to do that as this question seems to not been asked on the internet so far.\n",
        "\n",
        "A very large part of the effort put into this project was trying to find out\n",
        "what library is used as an intermediate language to move DAGs between\n",
        "the other ones and while nx seems the most likely candidate such unification has (probably) not happened yet.\n",
        "\n",
        "nx methods should be callable on pgmpy but that doesn't seem to be very\n",
        "consistent. We still managed to build a graph and plot it to check whether there has been some error in moving the graph by hand.\n",
        "\n",
        "born_an_even_day is not plotted because it has no edges but we did add it\n",
        "to the model explicitely so there is no need to double check using the plot"
      ],
      "metadata": {
        "id": "opKQYjrYbFH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We manually copy all the edges from the previously discovered graph\n",
        "edges = [\n",
        "    ('Smoking', 'Lung_cancer'),\n",
        "    ('Peer_Pressure', 'Smoking'),\n",
        "    ('Smoking', 'Yellow_Fingers'),\n",
        "    ('Genetics', 'Lung_cancer'),\n",
        "    ('Genetics', 'Attention_Disorder'),\n",
        "    ('Attention_Disorder', 'Car_Accident'),\n",
        "    ('Lung_cancer', 'Coughing'),\n",
        "    ('Lung_cancer', 'Fatigue'),\n",
        "    ('Fatigue', 'Car_Accident'),\n",
        "    ('Anxiety', 'Smoking'),\n",
        "    ('Coughing', 'Fatigue'),\n",
        "    ('Allergy', 'Coughing')\n",
        "]\n",
        "\n",
        "# We turn our edge list into a model\n",
        "model = DiscreteBayesianNetwork (edges)\n",
        "model.add_node('Born_an_Even_Day')\n",
        "\n",
        "# We inspect the graph\n",
        "print(list(model.nodes()))\n",
        "print(list(model.edges()))\n",
        "\n",
        "# We plot the graph\n",
        "nx_graph = nx.DiGraph(model.edges())\n",
        "nx.draw(nx_graph, with_labels=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R3fgAwpuSbJ3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute cdps"
      ],
      "metadata": {
        "id": "TU8e_fyebKRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#given the model pgmpy is pretty straightforward in its computation of the cpds\n",
        "model.fit(data)\n",
        "\n",
        "# we print the cdps to compare them with the ground truth.\n",
        "for cpd in model.get_cpds():\n",
        "    print(f\"CPD of {cpd.variable}:\")\n",
        "    print(cpd)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UgWHle5i6MjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####Table with the cdps taken from LUCAS against the ones we found out.\n",
        "The largest difference (0.07545) is in P(Fatigue=T|Lung cancer=F, Coughing=T)\n",
        "that is pretty big, but the other ones don't go above 0.03.\n",
        "The results overall seem satisfying\n",
        "\n",
        "P(Anxiety=T)=0.64277\t                              0.6305\n",
        "\n",
        "P(Peer Pressure=T)=0.32997\t                      \t0.3415\n",
        "\n",
        "P(Smoking=T|Peer Pressure=F, Anxiety=F)=0.43118\t\t  0.430327868852459\n",
        "\n",
        "P(Smoking=T|Peer Pressure=T, Anxiety=F)=0.74591\t\t  0.7131474103585658\n",
        "\n",
        "P(Smoking=T|Peer Pressure=F, Anxiety=T)=0.8686\t\t  0.8685162846803377\n",
        "\n",
        "P(Smoking=T|Peer Pressure=T, Anxiety=T)=0.91576\t\t  0.9166666666666666\n",
        "\n",
        "\n",
        "P(Yellow Fingers=T|Smoking=F)=0.23119               0.22424242424242424\n",
        "\n",
        "P(Yellow Fingers=T|Smoking=T)=0.95372               0.9654485049833887\n",
        "\n",
        "P(Genetics=T)=0.15953                               0.1395\n",
        "\n",
        "P(Lung cancer=T|Genetics=F, Smoking=F)=0.23146\t\t\t0.2517482517482518\n",
        "\n",
        "P(Lung cancer=T|Genetics=T, Smoking=F)=0.86996\t\t\t0.8939393939393939\n",
        "\n",
        "P(Lung cancer=T|Genetics=F, Smoking=T)=0.83934\t\t\t0.8227554179566563\n",
        "\n",
        "P(Lung cancer=T|Genetics=T, Smoking=T)=0.99351\t\t  1.0\n",
        "\n",
        "P(Attention Disorder=T|Genetics=F)=0.28956\t\t      0.27193492155723414\n",
        "\n",
        "P(Attention Disorder=T|Genetics=T)=0.68706\t\t      0.6344086021505376\n",
        "\n",
        "P(Born an Even Day=T)=0.5\t\t\t                      0.4895\n",
        "\n",
        "P(Allergy=T)=0.32841\t\t\t                          0.343\n",
        "\n",
        "P(Coughing=T|Allergy=F, Lung cancer=F)=0.1347\t\t    0.1352112676056338\n",
        "\n",
        "P(Coughing=T|Allergy=T, Lung cancer=F)=0.64592\t\t  0.6435643564356436\n",
        "\n",
        "P(Coughing=T|Allergy=F, Lung cancer=T)=0.7664\t\t    0.7705943691345151\n",
        "\n",
        "P(Coughing=T|Allergy=T, Lung cancer=T)=0.99947\t\t\t1.0\n",
        "\n",
        "P(Fatigue=T|Lung cancer=F, Coughing=F)=0.35212\t\t  0.35883905013192613\n",
        "\n",
        "P(Fatigue=T|Lung cancer=T, Coughing=F)=0.56514\t\t  0.5454545454545454\n",
        "\n",
        "P(Fatigue=T|Lung cancer=F, Coughing=T)=0.80016\t\t  0.7247191011235955\n",
        "\n",
        "P(Fatigue=T|Lung cancer=T, Coughing=T)=0.89589\t\t  0.8904333605887162\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=F, Fatigue=F)=0.2274\t\t  0.21351351351351353\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=T, Fatigue=F)=0.779\t\t    0.7628205128205128\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=F, Fatigue=T)=0.78861\t\t\t0.7857868020304568\n",
        "\n",
        "P(Car Accident=T|Attention Disorder=T, Fatigue=T)=0.97169\t\t  0.9693251533742331\n"
      ],
      "metadata": {
        "id": "g7CTTDECNTRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional and Interventional Estimate\n",
        "\n",
        "What is a valid adjustment set to estimate the causal effect of `Lung_cancer` on `Car Accident`? Justify your choice and compute the Average Treatment Effect. Then, compute the difference between conditional expectations and discuss whether they agree or not."
      ],
      "metadata": {
        "id": "YrQiK-ThVfKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dowhy import CausalModel"
      ],
      "metadata": {
        "id": "zVU1BCFgIA_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The adjustment set is composed only by the node Genetics, as it is the only parent of X that allows for a path to Y which doesn't include X (that is, the one through attention_disorder)"
      ],
      "metadata": {
        "id": "1IXUQLbJFjFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Car Accident conditioned only on Lung cancer\n",
        "We compute the difference between the average value of car accident given lung cancer and given no lung cancer and check the difference between those values\n",
        "\n",
        "$E[\\text{CarAccident} \\mid \\text{LungCancer = 1}]-E[\\text{CarAccident} \\mid \\text{LungCancer = 0}]$"
      ],
      "metadata": {
        "id": "Pw6z6oHvbXVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the conditional means\n",
        "mean_Y_given_X1 = data.loc[data['Lung_cancer'] == 1, 'Car_Accident'].mean()\n",
        "mean_Y_given_X0 = data.loc[data['Lung_cancer'] == 0, 'Car_Accident'].mean()\n",
        "\n",
        "# Compute the difference between the two\n",
        "conditional_diff = mean_Y_given_X1 - mean_Y_given_X0\n",
        "print(mean_Y_given_X1)\n",
        "print(mean_Y_given_X0)\n",
        "print(conditional_diff)"
      ],
      "metadata": {
        "id": "jT4BvJHLwfMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATE computation using Genetics\n",
        "We compute\n",
        "\n",
        "$\\sum_{w \\in \\{0,1\\}} E[\\text{CarAccident} \\mid \\text{LungCancer = 1}, \\text{Genetics}=w] \\cdot P(\\text{Genetics}=w)$\n",
        "\n",
        "and\n",
        "\n",
        "$\\sum_{w \\in \\{0,1\\}} E[\\text{CarAccident} \\mid \\text{LungCancer = 0}, \\text{Genetics}=w] \\cdot P(\\text{Genetics}=w)$\n",
        "\n",
        "Using the values from the dataset: the difference between them is the average treatment effect, adjusted for genetics"
      ],
      "metadata": {
        "id": "SF8P58yfbfmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We compute a matrix of lung cancer and genetics 0s and 1s and count how many points\n",
        "# have car accident = 1 in each square of the matrix\n",
        "X1_G1 = data.loc[(data['Lung_cancer'] == 1) & (data['Genetics'] == 1), 'Car_Accident'].mean()\n",
        "X1_G0 = data.loc[(data['Lung_cancer'] == 1) & (data['Genetics'] == 0), 'Car_Accident'].mean()\n",
        "X0_G1 = data.loc[(data['Lung_cancer'] == 0) & (data['Genetics'] == 1), 'Car_Accident'].mean()\n",
        "X0_G0 = data.loc[(data['Lung_cancer'] == 0) & (data['Genetics'] == 0), 'Car_Accident'].mean()\n",
        "\n",
        "# We compute the marginal distribution of Genetics\n",
        "P_G1 = (data['Genetics'] == 1).mean()\n",
        "P_G0 = 1 - P_G1\n",
        "\n",
        "# We adjust expectations according to the distribution of Genetics\n",
        "Y_do_X1 = X1_G1 * P_G1 + X1_G0 * P_G0\n",
        "Y_do_X0 = X0_G1 * P_G1 + X0_G0 * P_G0\n",
        "\n",
        "# We compute the difference between the points where\n",
        "adjusted_ate = Y_do_X1 - Y_do_X0\n",
        "print(Y_do_X1)\n",
        "print(Y_do_X0)\n",
        "print(adjusted_ate)"
      ],
      "metadata": {
        "id": "v5exmb_Ehryz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The result we just got from our ATE computation is extremely close to the one we had gotten without conditioning on genetics\n",
        "\n",
        "This implies that Genetics is not actually a good adjustment set (or that there is very little effect to adjust for), as we had hypothized"
      ],
      "metadata": {
        "id": "PMIELmzeMAL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we count in absolute terms the relationships between genetics and lung cancer\n",
        "print(len(data.loc[(data['Genetics'] == 1) & (data['Lung_cancer'] == 0)]))\n",
        "print(len(data.loc[(data['Genetics'] == 1) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Genetics'] == 0) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Genetics'] == 0) & (data['Lung_cancer'] == 0)]))\n"
      ],
      "metadata": {
        "id": "Y38Q0_yskbaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the vast majority of values in which Genetics is 1 also have lung cancer equal to 1. This might not be the best sign of a good adjustment set.\n",
        "Since attention disorder also blocks the backdoor path (as it is the only other child of genetics), let's try exploring it too"
      ],
      "metadata": {
        "id": "z15W55GpQtwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we count in absolute terms the relationships between lung cancer and attention disorder\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 1) & (data['Lung_cancer'] == 0)]))\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 1) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 0) & (data['Lung_cancer'] == 1)]))\n",
        "print(len(data.loc[(data['Attention_Disorder'] == 0) & (data['Lung_cancer'] == 0)]))"
      ],
      "metadata": {
        "id": "zuovi5_aNEOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATE using attention disorder\n",
        "Attention disorder seems to not share the same problem as genetics. Let's try running the same code as before using it as our adjustment set"
      ],
      "metadata": {
        "id": "ZfSwlwWLVphu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We compute a matrix of lung cancer and attention disorder 0s and 1s and count how many points\n",
        "# have car accident = 1 in each square of the matrix\n",
        "X1_A1 = data.loc[(data['Lung_cancer'] == 1) & (data['Attention_Disorder'] == 1), 'Car_Accident'].mean()\n",
        "X1_A0 = data.loc[(data['Lung_cancer'] == 1) & (data['Attention_Disorder'] == 0), 'Car_Accident'].mean()\n",
        "X0_A1 = data.loc[(data['Lung_cancer'] == 0) & (data['Attention_Disorder'] == 1), 'Car_Accident'].mean()\n",
        "X0_A0 = data.loc[(data['Lung_cancer'] == 0) & (data['Attention_Disorder'] == 0), 'Car_Accident'].mean()\n",
        "\n",
        "# We compute the marginal distribution of attention disorder\n",
        "P_A1 = (data['Attention_Disorder'] == 1).mean()\n",
        "P_A0 = 1 - P_G1\n",
        "\n",
        "# We adjust expectations according to the distribution of attention disorder\n",
        "Y_do_X1 = X1_A1 * P_A1 + X1_A0 * P_A0\n",
        "Y_do_X0 = X0_A1 * P_A1 + X0_A0 * P_A0\n",
        "\n",
        "# We compute the difference between the points where\n",
        "adjusted_ate = Y_do_X1 - Y_do_X0\n",
        "print(Y_do_X1)\n",
        "print(Y_do_X0)\n",
        "print(adjusted_ate)"
      ],
      "metadata": {
        "id": "heGVbiBRLN7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effect now is slightly lower than the one we had previously estimated using genetics. It is still very similar to the naive one but not uncanningly so"
      ],
      "metadata": {
        "id": "6ts2cPf4V-6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATE estimation using dowhy\n",
        "\n",
        " dowhy has its own ate function, so we try it to compare its results with ours\n",
        " we first need to build the model. For some reason i'm unable to utilize the\n",
        " one i had originally built during the structure learning (from my understanding casuallearn is part of pywhy) but luckily CausalModel accepts nx graphs as input\n",
        "\n",
        " Printing the result confirms us that genetics was indeed the adjustment_set\n",
        " frontdoor is another method that is explained in the book that requires\n",
        " a different adjustment set from the one we use in backdoor\n",
        "\n"
      ],
      "metadata": {
        "id": "4swTifrTbUkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we build the dowhy causalmodel using our graph and data\n",
        "modeldw = CausalModel(\n",
        "    data=data,\n",
        "    treatment='Lung_cancer',\n",
        "    outcome='Car_Accident',\n",
        "    graph=nx_graph\n",
        ")\n",
        "\n",
        "# i don't really understand why but the estimate is a required input in\n",
        "# the estimate_effect method, which we will use later.\n",
        "# running this function also helps us confirm our adjustment sets\n",
        "estimand = modeldw.identify_effect()\n",
        "\n",
        "# we print this\n",
        "print(estimand)\n"
      ],
      "metadata": {
        "id": "cfK1vieqwYzG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " We try both the backdoor and frontdoor methods as they are already implemented in dowhy\n"
      ],
      "metadata": {
        "id": "hS8X-v1vaUGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we run the estimation function using genetics, as suggested by the estimand\n",
        "estimatebackgen = modeldw.estimate_effect(\n",
        "    estimand,\n",
        "    method_name=\"backdoor.linear_regression\",\n",
        "    method_params={\"backdoor_adjustment_set\": [\"Genetics\"]}\n",
        ")\n",
        "\n",
        "# we also run using attention disorder, as it had given us more promising results\n",
        "estimatebackad = modeldw.estimate_effect(\n",
        "    estimand,\n",
        "    method_name=\"backdoor.linear_regression\",\n",
        "    method_params={\"backdoor_adjustment_set\": [\"Attention_Disorder\"]}\n",
        ")\n",
        "\n",
        "# we run the frontdoor effect estimation function using the values we obtained\n",
        "# from identify_effect\n",
        "estimatefront = modeldw.estimate_effect(\n",
        "    estimand,\n",
        "    method_name=\"frontdoor.linear_regression\",\n",
        "    method_params={\"frontdoor_adjustment_set\": [\"Coughing\",  \"Fatigue\"]}\n",
        ")\n",
        "\n",
        "# we print the results\n",
        "print(estimatebackgen.value, '\\n', estimatebackad.value, '\\n', estimatefront.value)"
      ],
      "metadata": {
        "id": "QecAg1BMaS-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All functions give us the same result.\n",
        "In the backdoor method the algorithm is probably able to use whatever set completely blocks the path, which makes this result easily predictable, while in the frontdoor method it is more interesting to see it get the same results.\n",
        "\n",
        "All results are also generally much closer to the one we had found using attention disorder as our adjustment set.\n"
      ],
      "metadata": {
        "id": "kDumGkDqWXVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions\n",
        "Despite my initial enthusiasm genetics proved itself not to be the correct adjustment set. The fact that the edge between it and attention disorder had to be manually directed only prompted me to think that there is a weak causal relationship between genetics and attention disorder, which in my eyes explained the weak effect genetics had as an adjustment set. Had i been more open minded i would have lost much less time exploring libraries.\n",
        "\n",
        " Regarding the slight difference between dowhy's and my estimations, given that we use the same data and refer to the same function in our computation, there must be something something i don't expect to be happening in the linear regression method of dowhy (looking at the linear_regression_estimator function, which is called in dowhy/causal_model it is unclear what the library does with this linear estimator but given the nature of the problem there are not many guesses). This is a problem i explored too deeply before trying out attention disorder as an adjustment set, but it might still be somewhat interesting to note.\n"
      ],
      "metadata": {
        "id": "YbPqaCsuJPon"
      }
    }
  ]
}